
from .state import GraphState
from .base import BaseNode
import pymupdf
import os
from PIL import Image
import matplotlib.pyplot as plt
import os
from typing_extensions import Annotated, TypedDict
import json
import pickle
import matplotlib

# GUI ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” Agg ë°±ì—”ë“œ ì‚¬ìš©
#matplotlib.use('Agg')
def show_graph(graph, title="graph", display=True, output_path="graph.png"):
    # graph ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ ê°ì²´ë¡œ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥
    png_data = graph.get_graph(xray=True).draw_mermaid_png()  # ì´ë¯¸ì§€ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ìƒì„±
    with open(output_path, "wb") as f:
        f.write(png_data)  # ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥

    print("Graph image saved as '{output_path}'. Displaying the image...")

    # display=Trueì¼ ë•Œë§Œ í™”ë©´ì— í‘œì‹œ
    if display:
        try:
            # ì €ì¥ëœ ì´ë¯¸ì§€ë¥¼ ì½ì–´ì„œ matplotlibìœ¼ë¡œ í‘œì‹œ
            image = Image.open("graph.png")
            plt.imshow(image)
            plt.axis("off")  # ì¶• ìˆ¨ê¸°ê¸°
            # í˜„ì¬ figureì˜ canvas ë§¤ë‹ˆì €ë¥¼ í†µí•´ ìœˆë„ìš° íƒ€ì´í‹€ ì„¤ì •
            plt.gcf().canvas.manager.set_window_title(title)            
            plt.show()
        except Exception as e:
            print("ê·¸ë˜í”„ í‘œì‹œ ê³¼ì •ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)


import xml.etree.ElementTree as ET


def load_xml_files(directory, encoding="utf-8"):
    """
    ì£¼ì–´ì§„ ë””ë ‰í† ë¦¬ì—ì„œ .xml íŒŒì¼ì„ ë¡œë“œí•˜ê³ , XML ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        directory (str): XML íŒŒì¼ì´ í¬í•¨ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        encoding (str): XML íŒŒì¼ì„ ë””ì½”ë”©í•  ë•Œ ì‚¬ìš©í•  ì¸ì½”ë”©. ê¸°ë³¸ê°’ì€ 'utf-8'.

    Returns:
        list: íŒŒì‹±ëœ XML ë‚´ìš©ì˜ ë¦¬ìŠ¤íŠ¸. ê° í•­ëª©ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ íŒŒì¼ ì´ë¦„ê³¼ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    h1_docs = []
    for file in os.listdir(directory):
        if file.endswith(".xml"):  # XML íŒŒì¼ë§Œ ì²˜ë¦¬
            file_path = os.path.join(directory, file)
            try:
                with open(file_path, "rb") as f:  # ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì½ê¸°
                    content = f.read()
                    root = ET.fromstring(content)  # XML íŒŒì‹±
                    text = ET.tostring(root, encoding="unicode")  # ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜
                    h1_docs.append({"file": file, "content": text})
            except Exception as e:
                print(f"Error parsing file {file_path}: {e}")
    return h1_docs

class SplitPDFFilesNode(BaseNode):

    def __init__(self, batch_size=10, test_page=None, **kwargs):
        super().__init__(**kwargs)
        self.name = "SplitPDFNode"
        self.batch_size = batch_size
        self.test_page = test_page

    def execute(self, state: GraphState) -> GraphState:
        """
        ì…ë ¥ PDFë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ PDF íŒŒì¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        :param state: GraphState ê°ì²´, PDF íŒŒì¼ ê²½ë¡œì™€ ë°°ì¹˜ í¬ê¸° ì •ë³´ë¥¼ í¬í•¨
        :return: ë¶„í• ëœ PDF íŒŒì¼ ê²½ë¡œ ëª©ë¡ì„ í¬í•¨í•œ GraphState ê°ì²´
        """
        # PDF íŒŒì¼ ê²½ë¡œì™€ ë°°ì¹˜ í¬ê¸° ì¶”ì¶œ
        filepath = state["filepath"]

        # PDF íŒŒì¼ ì—´ê¸°
        input_pdf = pymupdf.open(filepath)
        num_pages = len(input_pdf)
        self.log(f"íŒŒì¼ì˜ ì „ì²´ í˜ì´ì§€ ìˆ˜: {num_pages} Pages.")

        if self.test_page is not None:
            if self.test_page < num_pages:
                num_pages = self.test_page

        ret = []
        # PDF ë¶„í•  ì‘ì—… ì‹œì‘
        for start_page in range(0, num_pages, self.batch_size):
            # ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ê³„ì‚° (ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡)
            end_page = min(start_page + self.batch_size, num_pages) - 1

            # ë¶„í• ëœ PDF íŒŒì¼ëª… ìƒì„±
            input_file_basename = os.path.splitext(filepath)[0]
            output_file = f"{input_file_basename}_{start_page:04d}_{end_page:04d}.pdf"
            self.log(f"ë¶„í•  PDF ìƒì„±: {output_file}")

            # ìƒˆë¡œìš´ PDF íŒŒì¼ ìƒì„± ë° í˜ì´ì§€ ì‚½ì…
            with pymupdf.open() as output_pdf:
                output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)
                output_pdf.save(output_file)
                ret.append(output_file)

        # ì›ë³¸ PDF íŒŒì¼ ë‹«ê¸°
        input_pdf.close()

        # ë¶„í• ëœ PDF íŒŒì¼ ê²½ë¡œ ëª©ë¡ì„ í¬í•¨í•œ GraphState ê°ì²´ ë°˜í™˜
        return GraphState(split_filepaths=ret)

# LangChainì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from langchain_core.prompts import PromptTemplate
from pydantic import Field, BaseModel
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_openai import ChatOpenAI
from langchain_teddynote.models import get_model_name, LLMs
import re
import markdown
# GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ëª¨ë¸ëª…ì„ ê°€ì ¸ì˜µë‹ˆë‹¤


# ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹´ê¸° ìœ„í•œ Pydantic ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class TranslatedText(BaseModel):
    # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  í•„ë“œ ì •ì˜
    translated_text: str = Field(description="The translated text of the given text")

class TranslateNode(BaseNode):

    def __init__(self, model_name = get_model_name(LLMs.GPT4), verbose=False, language="Korean", **kwargs):
        super().__init__(**kwargs)
        self.name = "TranslateNode"
        super().__init__(verbose=verbose, **kwargs)
        self.model_name = model_name
        self.language="Korean"
        # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  í•„ë“œ ì •ì˜
        # Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì¶œë ¥ íŒŒì„œ ìƒì„±
        output_parser = PydanticOutputParser(pydantic_object=TranslatedText)
        # ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt = PromptTemplate.from_template(
            """You are a translation expert. Translate the <given_text> into Korean.
        [IMPORTANT] Keep the <given_text>'s markdown format.

        ###

        <given_text>
        {text}
        </given_text>"""
        )
        # ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •
        self.llm = ChatOpenAI(model=model_name, temperature=0).with_structured_output(TranslatedText)

        # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±
        self.chain = self.prompt | self.llm
    """
    ë²ˆì—­ ëª¨ë“ˆì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ìƒíƒœ(state)ì˜ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.

    Args:
        state (GraphState): íŒŒì‹±ëœ ìƒíƒœ ê°ì²´

    Returns:
        dict: ë²ˆì—­ëœ ìš”ì†Œë“¤ì´ í¬í•¨ëœ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
    """


    def execute(self, state: GraphState):
        # ë²ˆì—­ì´ í•„ìš”í•œ ìš”ì†Œë“¤ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        translated_elements = []
        # ìƒíƒœì—ì„œ ë²ˆì—­ì´ í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ì˜ ìš”ì†Œë“¤ë§Œ ì„ íƒ
        for element in state["elements_from_parser"]:
            # ë²ˆì—­ì´ í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ë“¤ì„ ì§€ì •
            if element["category"] in [
                "paragraph",
                "index",
                "heading1",
                "header",
                "footer",
                "caption",
                "list",
                "footnote",
            ]:
                translated_elements.append(element)

        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (í•œ ë²ˆì— ì²˜ë¦¬í•  ìš”ì†Œ ìˆ˜)
        BATCH_SIZE = 50
        # ë²ˆì—­ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_translated_results = []
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë²ˆì—­ ì²˜ë¦¬
        for i in range(0, len(translated_elements), BATCH_SIZE):
            # í˜„ì¬ ë°°ì¹˜ì˜ ìš”ì†Œë“¤ ì¶”ì¶œ
            batch = translated_elements[i : i + BATCH_SIZE]
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch_data = [{"text": text["content"]["markdown"]} for text in batch]
            # ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì •
            trial = 3
            while trial > 0:
                try:
                    # ë°°ì¹˜ ë²ˆì—­ ì‹¤í–‰
                    batch_results = self.chain.batch(batch_data)
                    break
                except Exception as e:
                    print(e)
                    trial -= 1
                    continue
            # ë²ˆì—­ ê²°ê³¼ ì¶œë ¥
            for result in batch_results:
                print(result)
            # ì „ì²´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— í˜„ì¬ ë°°ì¹˜ ê²°ê³¼ ì¶”ê°€
            all_translated_results.extend(batch_results)

        # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ìš”ì†Œì— ì—…ë°ì´íŠ¸
        for i, result in enumerate(all_translated_results):
            translated_elements[i]["content"]["markdown"] = result.translated_text
            if self.language == "Korean":
                new_html = markdown.markdown(result.translated_text)
                new_html = new_html.replace("&lt;given_text&gt;", "").replace("&lt;/given_text&gt;", "")
                new_html = new_html.replace("<code>","").replace("</code>","").replace("<pre>","").replace("</pre>","")
                translated_elements[i]["content"]["html"] = new_html


        # ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë°˜í™˜
        return GraphState(elements_from_parser= state["elements_from_parser"], language="Korean")

def save_graph(save_file_name, graph):
    """
    í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    Args:
        save_file_name (str): ê·¸ë˜í”„ ìƒíƒœë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        graph (StateGraph): ì €ì¥í•  ê·¸ë˜í”„ ê°ì²´
    """
    try:
        with open(save_file_name, "wb") as f:
            pickle.dump(graph, f)
        print(f"âœ… ê·¸ë˜í”„ ìƒíƒœ ì €ì¥ ì™„ë£Œ: {save_file_name}")
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_graph(save_file_name):
    """
    ì €ì¥ëœ ê·¸ë˜í”„ ìƒíƒœë¥¼ íŒŒì¼ì—ì„œ ë³µì›í•©ë‹ˆë‹¤.
    Args:
        save_file_name (str): ê·¸ë˜í”„ ìƒíƒœê°€ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    Returns:
        StateGraph: ë³µì›ëœ ê·¸ë˜í”„ ê°ì²´
    """
    if not os.path.exists(save_file_name):
        print(f"âŒ ê·¸ë˜í”„ ìƒíƒœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {save_file_name}")
        return None

    try:
        with open(save_file_name, "rb") as f:
            graph = pickle.load(f)
        print(f"ğŸ”„ ê·¸ë˜í”„ ìƒíƒœ ë³µì› ì™„ë£Œ: {save_file_name}")
        return graph
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ìƒíƒœ ë³µì› ì‹¤íŒ¨: {e}")
        return None

    
    
def save_state(state_file, metadata, chunk_msg):
    """
    í˜„ì¬ ìƒíƒœë¥¼ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    Args:
        metadata (dict): ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ì˜ ë©”íƒ€ë°ì´í„°
        chunk_msg (str): í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë©”ì‹œì§€
    """
    state = {
        "node": metadata["langgraph_node"],
        "chunk_msg": chunk_msg.content,
    }
    with open(state_file, "w") as f:
        json.dump(state, f)
    print(f"âœ… ìƒíƒœ ì €ì¥ ì™„ë£Œ: {state}")

def load_state(state_file):
    """
    ì €ì¥ëœ ìƒíƒœë¥¼ JSON íŒŒì¼ì—ì„œ ë³µì›í•©ë‹ˆë‹¤.
    Returns:
        dict: ì €ì¥ëœ ìƒíƒœ ì •ë³´
    """
    if os.path.exists(state_file):
        with open(state_file, "r") as f:
            state = json.load(f)
        print(f"ğŸ”„ ìƒíƒœ ë³µì› ì™„ë£Œ: {state}")
        return state
    return None