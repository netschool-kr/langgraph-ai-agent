import os
import sys
# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath("."))
from typing import Any, Dict, List, Callable
from docgraph.utils import SplitPDFFilesNode, TranslateNode
from docgraph.state import GraphState
from docgraph.upstage import (
    DocumentParseNode,
    PostDocumentParseNode,
    WorkingQueueNode,
    continue_parse,
)
from langgraph.graph import StateGraph,START, END
from langgraph.checkpoint.memory import MemorySaver

from docgraph.preprocessing import (
    CreateElementsNode,
    MergeEntityNode,
    ReconstructElementsNode,
    LangChainDocumentNode,
)
from docgraph.export import ExportHTML, ExportMarkdown, ExportTableCSV, ExportImage
from docgraph.extractor import (
    PageElementsExtractorNode,
    ImageEntityExtractorNode,
    TableEntityExtractorNode,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import CompiledStateGraph


def create_parse_graph(
    batch_size: int = 30,
    test_page: int = None,
    verbose: bool = True,
):
    split_pdf_node = SplitPDFFilesNode(
        batch_size=batch_size, test_page=test_page, verbose=verbose
    )

    document_parse_node = DocumentParseNode(
        api_key=os.environ["UPSTAGE_API_KEY"], verbose=verbose
    )

    post_document_parse_node = PostDocumentParseNode(verbose=verbose)
    working_queue_node = WorkingQueueNode(verbose=verbose)

    # LangGraph ìƒì„±
    workflow = StateGraph(GraphState)

    # ë…¸ë“œë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
    workflow.add_node("split_pdf_node", split_pdf_node)
    workflow.add_node("document_parse_node", document_parse_node)
    workflow.add_node("post_document_parse_node", post_document_parse_node)
    workflow.add_node("working_queue_node", working_queue_node)

    # ê° ë…¸ë“œë“¤ì„ ì—°ê²°í•©ë‹ˆë‹¤.
    workflow.add_edge("split_pdf_node", "working_queue_node")
    workflow.add_conditional_edges(
        "working_queue_node",
        continue_parse,
        {True: "document_parse_node", False: "post_document_parse_node"},
    )
    workflow.add_edge("document_parse_node", "working_queue_node")

    workflow.set_entry_point("split_pdf_node")

    document_parse_graph = workflow.compile(checkpointer=MemorySaver())
    return document_parse_graph

# ì´ ê·¸ë˜í”„ëŠ” ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë‚´ë³´ë‚´ëŠ”(export) ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•œë‹¤.
def create_export_graph(
    ignore_new_line_in_text=True,
    show_image_in_markdown=False,
    verbose=True
):
    # GraphStateë¥¼ ë‹¤ë£¨ëŠ” StateGraph ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
    export_graph = StateGraph(GraphState)

    # ì´ë¯¸ì§€, HTML, Markdown, CSVë¡œ ë¬¸ì„œë¥¼ ë‚´ë³´ë‚´ëŠ” ë…¸ë“œë“¤ì„ ìƒì„±
    # verbose ì˜µì…˜ì„ í†µí•´ ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í• ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.
    export_image = ExportImage(verbose=verbose)
    export_html = ExportHTML(
        ignore_new_line_in_text=ignore_new_line_in_text, verbose=verbose
    )
    export_markdown = ExportMarkdown(
        ignore_new_line_in_text=ignore_new_line_in_text,
        show_image=show_image_in_markdown,
        verbose=verbose,
    )
    export_table_csv = ExportTableCSV(verbose=verbose)


    # ê·¸ë˜í”„ì— ë…¸ë“œ ë“±ë¡
    # ê° ë…¸ë“œëŠ” ë¬¸ì„œë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
    export_graph.add_node("export_image", export_image)
    export_graph.add_node("export_html", export_html)
    export_graph.add_node("export_markdown", export_markdown)
    export_graph.add_node("export_table_to_csv", export_table_csv)

    # ë…¸ë“œ ê°„ ì—£ì§€ ì—°ê²°
    # export_image ë…¸ë“œê°€ ì‹¤í–‰ëœ í›„, ê²°ê³¼ë¥¼ HTML, Markdown, CSV ë‚´ë³´ë‚´ê¸° ë…¸ë“œë¡œ ì „ë‹¬í•œë‹¤.
    export_graph.add_edge("export_image", "export_html")
    export_graph.add_edge("export_html", "export_markdown")
    export_graph.add_edge("export_markdown", "export_table_to_csv")
    export_graph.add_edge("export_table_to_csv", END)

    export_graph.set_entry_point("export_image")

    # ê·¸ë˜í”„ì˜ ì‹œì‘ ë…¸ë“œë¥¼ export_imageë¡œ ì„¤ì •
    # ì¦‰, ì´ë¯¸ì§€ë¥¼ ë‚´ë³´ë‚´ëŠ” ë‹¨ê³„ë¶€í„° ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹œì‘í•œë‹¤.
    c_graph = export_graph.compile(checkpointer=MemorySaver())
    return c_graph

from langchain_teddynote.models import get_model_name, LLMs

def create_translate_graph(verbose=True, model_name=get_model_name(LLMs.GPT4), language="Korean"):
    # GraphStateë¥¼ ë‹¤ë£¨ëŠ” StateGraph ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
    translate_graph = StateGraph(GraphState)
    translate = TranslateNode(model_name, language="Korean", verbose=verbose)


    # ê·¸ë˜í”„ì— ë…¸ë“œ ë“±ë¡
    # ê° ë…¸ë“œëŠ” ë¬¸ì„œë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
    translate_graph.add_node("translate", translate)

    # HTML, Markdown, CSV ë‚´ë³´ë‚´ê¸° ë…¸ë“œëŠ” ìµœì¢…ì ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì¢…ë£Œ(END)í•œë‹¤.
    translate_graph.add_edge("translate", END)

    translate_graph.set_entry_point("translate")

    # ê·¸ë˜í”„ì˜ ì‹œì‘ ë…¸ë“œë¥¼ export_imageë¡œ ì„¤ì •
    # ì¦‰, ì´ë¯¸ì§€ë¥¼ ë‚´ë³´ë‚´ëŠ” ë‹¨ê³„ë¶€í„° ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹œì‘í•œë‹¤.
    c_graph = translate_graph.compile(checkpointer=MemorySaver())
    return c_graph


def create_document_process_graph(batch_size=30, test_page=None, verbose=True):
    # PDFë¥¼ ë¶„í• í•˜ëŠ” ë…¸ë“œ ìƒì„±
    # batch_size=30: í•œë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜, test_page=None: ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬, verbose=True: ì²˜ë¦¬ ìƒí™© ì¶œë ¥
    split_pdf_node = SplitPDFFilesNode(batch_size=batch_size, test_page=test_page, verbose=verbose)
    
    # Upstage APIë¥¼ í†µí•´ ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ëŠ” ë…¸ë“œ
    # í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° UPSTAGE_API_KEYë¥¼ ì½ì–´ì™€ ì¸ì¦ í›„ íŒŒì‹± ì§„í–‰
    document_parse_node = DocumentParseNode(
        api_key=os.environ["UPSTAGE_API_KEY"], verbose=verbose
    )
    # ë¬¸ì„œ íŒŒì‹± ì´í›„ í›„ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ
    post_document_parse_node = PostDocumentParseNode(verbose=verbose)
    # ì‘ì—… íë¥¼ ê´€ë¦¬í•˜ëŠ” ë…¸ë“œë¡œ, ë‚¨ì€ í˜ì´ì§€ ì²˜ë¦¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ë¼ìš°íŒ…
    working_queue_node = WorkingQueueNode(verbose=verbose)

    # ì²« ë²ˆì§¸ ì›Œí¬í”Œë¡œìš° ìƒì„±
    graph = StateGraph(GraphState)
    
     # ë…¸ë“œ ë“±ë¡
    graph.add_node("split_pdf_node", split_pdf_node)
    graph.add_node("document_parse_node", document_parse_node)
    graph.add_node("post_document_parse_node", post_document_parse_node)
    graph.add_node("working_queue_node", working_queue_node)

    # ë…¸ë“œ ê°„ íë¦„ ì •ì˜
    # split_pdf_node â†’ working_queue_node: ë¶„í• ëœ PDF í˜ì´ì§€ ëª©ë¡ì„ íë¡œ ì „ë‹¬
    graph.add_edge("split_pdf_node", "working_queue_node")
    # working_queue_nodeì—ì„œ continue_parse í•¨ìˆ˜ë¥¼ í†µí•´ ë‚¨ì€ í˜ì´ì§€ ì²˜ë¦¬ ì—¬ë¶€ íŒë‹¨
    # True: ì•„ì§ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆìŒ â†’ document_parse_nodeë¡œ ì´ë™
    # False: ì²˜ë¦¬í•  í˜ì´ì§€ ì—†ìŒ â†’ post_document_parse_nodeë¡œ ì´ë™(íŒŒì‹± í›„ ë§ˆë¬´ë¦¬ ë‹¨ê³„)    
    graph.add_conditional_edges(
        "working_queue_node",
        continue_parse,
        {True: "document_parse_node", False: "post_document_parse_node"},
    )
    # document_parse_node ì²˜ë¦¬ í›„ ë‹¤ì‹œ working_queue_nodeë¡œ ëŒì•„ê°€ ë‹¤ìŒ í˜ì´ì§€ ì²˜ë¦¬ ì—¬ë¶€ íŒë‹¨    
    graph.add_edge("document_parse_node", "working_queue_node")
    # ê·¸ë˜í”„ ì‹œì‘ì  ì„¤ì •: split_pdf_nodeì—ì„œ ì›Œí¬í”Œë¡œìš° ì‹œì‘
    graph.set_entry_point("split_pdf_node")
    parser_graph = graph.compile()

    # í›„ì²˜ë¦¬ ë‹¨ê³„ì— í•„ìš”í•œ ë…¸ë“œ ìƒì„±
    # CreateElementsNode: íŒŒì‹± ê²°ê³¼ë¡œë¶€í„° ìš”ì†Œ(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ ë“±)ë¥¼ ì¶”ì¶œ/ìƒì„±
    create_elements_node = CreateElementsNode(verbose=verbose)
    # Export ê³„ì—´ ë…¸ë“œ: íŒŒì‹± ê²°ê³¼ë¬¼ì„ ë‹¤ì–‘í•œ í˜•ì‹(HTML, Markdown, CSV)ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ë…¸ë“œ
    export_html = ExportHTML(verbose=verbose)
    export_markdown = ExportMarkdown(verbose=verbose)
    export_table_csv = ExportTableCSV(verbose=verbose)
    
    # PageElementsExtractorNode: ê° í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ìš”ì†Œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡°ì  ì •ë³´ ì¶”ì¶œ
    page_elements_extractor_node = PageElementsExtractorNode(verbose=verbose)
    # ImageEntityExtractorNode: ì´ë¯¸ì§€ ê´€ë ¨ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œ
    image_entity_extractor_node = ImageEntityExtractorNode(verbose=verbose)
    # TableEntityExtractorNode: í…Œì´ë¸” ê´€ë ¨ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œ
    table_entity_extractor_node = TableEntityExtractorNode(verbose=verbose)
    
    # MergeEntityNode: ì´ë¯¸ì§€ë‚˜ í‘œ ë“± ì¶”ì¶œëœ ì—”í‹°í‹°ë“¤ì„ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì •ë¦¬
    merge_entity_node = MergeEntityNode(verbose=True)
    # ReconstructElementsNode: ë³‘í•©ëœ ì—”í‹°í‹°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ ë¬¸ì„œ êµ¬ì¡°ë¥¼ ì¬êµ¬ì„±
    reconstruct_elements_node = ReconstructElementsNode(verbose=True)
    # LangChainDocumentNode: ì¬êµ¬ì„±ëœ ë¬¸ì„œ ìš”ì†Œë¥¼ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
    # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•´ ê¸´ ë¬¸ì„œë¥¼ chunking    
    langchain_document_node = LangChainDocumentNode(
        verbose=True,
        splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0),
    )

    # í›„ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ìƒì„±
    post_process_graph = StateGraph(GraphState)

    # í›„ì²˜ë¦¬ ë‹¨ê³„ì— í•„ìš”í•œ ë…¸ë“œ ë“±ë¡
    # document_parse ë…¸ë“œì— ì•ì„œ ì»´íŒŒì¼í•œ parser_graphë¥¼ ì‚¬ìš©
    post_process_graph.add_node("document_parse", parser_graph)
    post_process_graph.add_node("create_elements_node", create_elements_node)
    post_process_graph.add_node("export_html", export_html)
    post_process_graph.add_node("export_markdown", export_markdown)
    post_process_graph.add_node("export_table_csv", export_table_csv)
    post_process_graph.add_node(
        "page_elements_extractor_node", page_elements_extractor_node
    )
    post_process_graph.add_node(
        "image_entity_extractor_node", image_entity_extractor_node
    )
    post_process_graph.add_node(
        "table_entity_extractor_node", table_entity_extractor_node
    )
    post_process_graph.add_node("merge_entity_node", merge_entity_node)
    post_process_graph.add_node(
        "reconstruct_elements_node", reconstruct_elements_node
    )
    post_process_graph.add_node("langchain_document_node", langchain_document_node)

    # ì—£ì§€ ì—°ê²° (í›„ì²˜ë¦¬ ë‹¨ê³„ì˜ íë¦„ ì •ì˜)
    # document_parse â†’ create_elements_node: íŒŒì‹±ì´ ì™„ë£Œëœ ë¬¸ì„œë¥¼ ìš”ì†Œí™”
    post_process_graph.add_edge("document_parse", "create_elements_node")
    # create_elements_nodeì—ì„œ ìƒì„±ëœ ìš”ì†Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ HTML, Markdown, CSV ë‚´ë³´ë‚´ê¸° ê°€ëŠ¥
    post_process_graph.add_edge("create_elements_node", "export_html")
    post_process_graph.add_edge("create_elements_node", "export_markdown")
    post_process_graph.add_edge("create_elements_node", "export_table_csv")
    # create_elements_node ê²°ê³¼ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ìš”ì†Œ ì¶”ì¶œ
    post_process_graph.add_edge(
        "create_elements_node", "page_elements_extractor_node"
    )
    # í˜ì´ì§€ ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ì™€ í…Œì´ë¸” ì¶”ì¶œ ë¶„ê¸°
    post_process_graph.add_edge(
        "page_elements_extractor_node", "image_entity_extractor_node"
    )
    post_process_graph.add_edge(
        "page_elements_extractor_node", "table_entity_extractor_node"
    )
    post_process_graph.add_edge("image_entity_extractor_node", "merge_entity_node")
    # HTML, Markdown, CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ ì‹œ ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ
    post_process_graph.add_edge("export_html", END)
    post_process_graph.add_edge("export_markdown", END)
    post_process_graph.add_edge("export_table_csv", END)
    # í…Œì´ë¸” ì—”í‹°í‹° ì¶”ì¶œ í›„ ì—”í‹°í‹° ë³‘í•© ë‹¨ê³„ë¡œ ì´ë™
    post_process_graph.add_edge("table_entity_extractor_node", "merge_entity_node")
    # ì—”í‹°í‹° ë³‘í•© ì™„ë£Œ í›„ ì¬êµ¬ì„± ë…¸ë“œë¡œ ì´ë™
    post_process_graph.add_edge("merge_entity_node", "reconstruct_elements_node")
    # ì¬êµ¬ì„±ëœ ìš”ì†Œë¥¼ LangChain Documentë¡œ ë³€í™˜ í›„ ì¢…ë£Œ
    post_process_graph.add_edge(
        "reconstruct_elements_node", "langchain_document_node"
    )
    post_process_graph.add_edge("langchain_document_node", END)

     # í›„ì²˜ë¦¬ ê·¸ë˜í”„ì˜ ì‹œì‘ ì§€ì ì„ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ ë‹¨ê³„ë¡œ ì„¤ì •
    post_process_graph.set_entry_point("document_parse")

     # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ë¥¼ ìœ„í•œ MemorySaver ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    memory = MemorySaver()
    # í›„ì²˜ë¦¬ ê·¸ë˜í”„ ì»´íŒŒì¼ ë° ë°˜í™˜
    c_graph = post_process_graph.compile(checkpointer=memory)
    return c_graph

def stream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.

    Returns:
        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.
    """
    prev_node = ""
    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
        curr_node = metadata["langgraph_node"]

        # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if not node_names or curr_node in node_names:
            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
            if callback:
                callback({"node": curr_node, "content": chunk_msg.content})
            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
            else:
                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                if curr_node != prev_node:
                    print("\n" + "=" * 50)
                    print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    print("- " * 25)
                print(chunk_msg.content, end="", flush=True)

            prev_node = curr_node

def format_namespace(namespace):
    return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"

def invoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.

    Returns:
        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.
    """


    # subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨
    for namespace, chunk in graph.stream(
        inputs, config, stream_mode="updates", subgraphs=True
    ):
        for node_name, node_chunk in chunk.items():
            # node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§
            if len(node_names) > 0 and node_name not in node_names:
                continue

            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
            if callback is not None:
                callback({"node": node_name, "content": node_chunk})
            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
            else:
                print("\n" + "=" * 50)
                formatted_namespace = format_namespace(namespace)
                if formatted_namespace == "root graph":
                    print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
                else:
                    print(
                        f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m in [\033[1;33m{formatted_namespace}\033[0m] ğŸ”„"
                    )
                print("- " * 25)

                # ë…¸ë“œì˜ ì²­í¬ ë°ì´í„° ì¶œë ¥
                for k, v in node_chunk.items():
                    if isinstance(v, BaseMessage):
                        v.pretty_print()
                    elif isinstance(v, list):
                        for list_item in v:
                            if isinstance(list_item, BaseMessage):
                                list_item.pretty_print()
                            else:
                                print(list_item)
                    elif isinstance(v, dict):
                        for node_chunk_key, node_chunk_value in node_chunk.items():
                            print(f"{node_chunk_key}:\n{node_chunk_value}")
                print("=" * 50)
